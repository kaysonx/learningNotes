## 从0开始学架构 - 3

###14. 高性能数据库集群：读写分离

> 并非所有系统都需要进行读写分离，根据“合适原则”的规定，先确认系统的业务量是否出现了数据库的性能问题。如果是，排查原因，如果通过优化MySQL语句 加缓存 优化中间件等方式能解决最好，如果还是达不到要求的性能指标，再读写分离
>
> 毕竟读写分离会引入新的复杂度，如数据不同步

要点：分散数据库读写操作的压力

基本原理：将数据库读写操作分散到不同的节点上

基本实现：

1. 数据库服务器搭建主从集群，一主一从、一主多从都可以
2. 数据库主机负责读写操作，从机只负责读操作
3. 数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据
4. 业务服务器将写操作发给数据库主机，将读操作发给数据库从机

区别于主备集群，从机是有任务的。而备机仅仅提供备份功能

设计复杂度：

1.主从复制延迟

常见解决方法：

1. 写操作后的读操作指定发给数据库主服务器 - 和业务强相关
2. 读从机失败后再读一次主机 - 二次读取，会增加主机的读操作压力
3. 关键业务读写操作全部指向主机，非关键业务采用读写分离
4. 加入缓存并设置过期时间，先从缓存获取 再从数据库中获得

2.分配机制

1. 程序代码封装
   * 代码中抽象一个数据访问层，用于实现读写操作分离和数据库服务器连接的管理
   * 实现简单 可以根据业务定制化功能
   * 不同语言无法通用 
   * 故障情况下 如果主从切换 **可能**需要所有系统都修改配置重启
   * 开源解决方案：
     * 淘宝TDDL
2. 中间件封装
   * 独立一套系统来实现读写分离 和 数据库服务器连接的管理
   * 能够支持多种编程语言，因为数据库中间件对业务服务器提供的是标准 SQL 接口
   * 数据库中间件要支持完整的 SQL 语法和数据库服务器的协议（例如，MySQL 客户端和服务器的连接协议），实现比较复杂，细节特别多，很容易出现 bug，需要较长的时间才能稳定
   * 数据库中间件自己不执行真正的读写操作，但所有的数据库操作请求都要经过中间件，中间件的性能要求也很高
   * 数据库主从切换对业务服务器无感知，数据库中间件可以探测数据库服务器的主从状态。例如，向某个测试表写入一条数据，成功的就是主机，失败的就是从机
   * 开源解决方案：
     * MySQL Router
     * 360 - Atlas

> 读写分离适用 单机并发 无法支撑并且 读的请求更多 的情形。在单机数据库情况下，表上加索引一般对 查询有优化作用 却 影响写入速度，读写分离后可以单独对读库进行优化，写库上减少索引，对读写的能力都有提升，且读的提升更多一些
>
> 不适用的情况:
>
> 1. 如果并发写入特别高，单机写入无法支撑，就不适合这种模式
> 2. 通过缓存技术或者程序优化能够满足要求

缓存一般应用于查询类业务上

### 15.高性能数据库集群：分库分表

要点：分散数据库存储的压力

存储能力成为瓶颈后造成的问题：

1. 数据量太大，读写的性能会下降，即使有索引，索引也会变得很大，性能同样会下降
2.  数据文件会变得很大，数据库备份和恢复需要耗费很长时间
3. 数据文件越大，极端情况下丢失数据的风险越高（例如，机房火灾导致数据库主备机都发生故障）

**业务分库**

定义：指的是按照业务模块将数据分散到不同的数据库服务器

带来的问题：

1. join问题 - 业务分库后，原本在同一个数据库中的表分散到不同数据库中，导致无法使用 SQL 的 join 查询
2. 事务问题 - 原本在同一个数据库中不同的表可以在同一个事务中修改，业务分库后，表分散到不同的数据库中，无法通过事务统一修改。虽然数据库厂商提供了一些分布式事务的解决方案（例如，MySQL 的 XA），但性能实在太低，与高性能存储的目标是相违背的
3. 成本问题 - 业务分库同时也带来了成本的代价，本来 1 台服务器搞定的事情，现在要 3 台，如果考虑备份，那就是 2 台变成了 6 台

小公司初创业务，不建议一开始这样拆分 原因如下：

1. 初创业务存在很大的不确定性，业务不一定能发展起来，业务开始的时候并没有真正的存储和访问压力，业务分库并不能为业务带来价值
2. 业务分库后，表之间的 join 查询、数据库事务无法简单实现了
3. 业务分库后，因为不同的数据要读写不同的数据库，代码中需要增加根据数据类型映射到不同数据库的逻辑，增加了工作量。而业务初创期间最重要的是快速实现、快速验证，业务分库会拖慢业务节奏

如果是成熟的大公司，可以在一开始设计就考虑好业务分库

**分表**

垂直分表

适合将表中某些不常用且占了大量空间的列拆分出去

引入的复杂性在于 表操作的数量要增加

水平分表

适合表行数特别大的表

引入的复杂性在于：

1. 路由
   1. 范围路由
   2. Hash路由
   3. 配置路由 - 独立路由表
2. join操作 - 需要多次join 然后合并结果
3. count操作 
   1. count相加 - 分表count然后相加
   2. 记录表数 - 新建表记录count
4. order by/group等操作

**实现方式**

分库分表 类似于 读写分离，包含“程序代码封装”和“中间件封装”



如何学习一个技术：

<ol>
<li><p>搭建一个单机伪集群，搭建完成后看看安装路径下的文件和目录，看看配置文件有哪些配置项，不同的配置项会有什么样的影响。</p>
</li>
<li><p>执行常用的操作，例如创建索引，插入、删除、查询文档，查看一下各种输出。</p>
</li>
<li><p>研究其<strong>基本原理</strong>，例如索引、分片、副本等，研究的时候要多思考，例如索引应该如何建，分片数量和副本数量对系统有什么影响等。</p>
</li>
<li><p>和其他类似系统对比，例如 Solr、Sphinx，研究其<strong>优点、缺点、适用场景</strong>。</p>
</li>
<li><p>模拟一个案例看看怎么应用。例如，假设我用 Elasticsearch 来存储淘宝的商品信息，我应该如何设计索引和分片。</p>
</li>
<li><p>查看业界使用的案例，思考一下别人为何这么用；看看别人测试的结果，大概了解性能范围。</p>
</li>
<li><p>如果某部分特别有兴趣或者很关键，可能去看源码，例如 Elasticsearch 的选举算法（我目前还没看 ^_^）。</p>
</li>
<li><p>如果确定要引入，会进行性能和可用性测试。</p>
</li>
</ol>

### 16.高性能NoSQL

关系型数据库的缺点：

1. 关系数据库存储的是行记录，无法存储数据结构
2. 关系数据库的 schema 扩展很不方便
3. 关系数据库在大数据场景下 I/O 较高 - 因为是整行数据读取进入内存操作
4. 关系数据库的全文检索功能比较弱 - 只能用like进行整表匹配

NoSQL - Not only SQL

NoSQL带来的优势，本质上是牺牲ACID中的某个或者某几个特性

常见的NoSQL:

<ul>
<li><p>K-V 存储：解决关系数据库无法存储数据结构的问题，以 Redis 为代表。</p>
</li>
<li><p>文档数据库：解决关系数据库强 schema 约束的问题，以 MongoDB 为代表。</p>
</li>
<li><p>列式数据库：解决关系数据库大数据场景下的 I/O 问题，以 HBase 为代表。</p>
</li>
<li><p>全文搜索引擎：解决关系数据库的全文搜索性能问题，以 Elasticsearch 为代表。</p>
</li>
</ul>

 **K-V存储**

优势：V支持数据结构 并 提供了对应的操作

不足：Redis只能保证隔离线和一致性(IC)，无法保证原子性和持久性(AD)

**文档数据库**

优势：

1. 新增字段简单
2. 历史数据不会出错
3. 很容易存储复杂数据

不足：

1. 不支持事务

2. 无法实现关系数据库的join操作

**列式数据库**

特点：数据库按列存储数据

优势：

1. 业务同时读取多个列时效率高，因为这些列都是按行存储在一起的，一次磁盘操作就能够把一行数据中的各个列都读取到内存中
2. 能够一次性完成对一行中的多个列的写操作，保证了针对行数据写操作的原子性和一致性；否则如果采用列存储，可能会出现某次写操作，有的列成功了，有的列失败了，导致数据不一致
3. 列式存储具备更高的存储压缩比 能节省更多的存储空间
4. 一把应用在离线的大数据分析和统计场景中

**全文搜索引擎**

使用倒排索引，以词为单位建立索引

需要将关系型数据按照对象的形式转换为JSON文档 然后将JSON文档输入全文搜索引擎进行索引

### 17.高性能缓存架构

 应用场景：

1. 需要经过复杂计算后得出的数据 存储系统无能为力
2. 读多写少的数据，存储系统有心无力

基本原理：讲可能重复使用的数据放到内存中，一次生成，多次使用。避免每次使用都去访问存储系统

设计要点：

1. 缓存穿透 - 即是缓存失效 压力直接倾泻在存储系统
   1. 存储数据不存在 - 
      * 数据确实不存在 - 如果查询存储系统没有找到 则直接设置一个默认值到缓存
      * 缓存数据生成耗费大量时间或者资源 
2. 缓存雪崩 - 缓存失效后引起系统性能急剧下降的情况
   1. 更新锁机制 - 对缓存更新操作进行加锁 避免失效后所有请求都去重新生成缓存 分布式环境注意分布式🔐 例如ZooKeeper
   2. 后台更新 - 使用单独的后台进程来更新缓存 缓存本身的有效期设置为永久，后台线程定时更新
      1. 后台线程还要频繁读取缓存 发现缓存被踢后就立刻更新缓存
      2. 业务线程发现缓存失效后，通过消息队列发送一条消息通知后台线程更新缓存
3. 缓存热点  - 热点数据 大量业务请求命中同一份缓存数据
   1. 复制多份缓存副本 讲请求分散到多个缓存服务器上

实现要点：程序代码或者独立中间件

### 18.单服务器高性能模式 - PPC与TPC

高性能架构设计：

1. 尽量提升单服务器的性能 讲单服务器的性能发挥到极致
2. 如果单服务器无法支撑性能，设计服务器集群方案

单服务器高性能的关键在于服务器采取的并发模型：

1. 服务器如何管理连接
2. 服务器如何处理请求

以上两个设计点都和操作系统的I/O模型及进程模型相关：

1. I/O模型：阻塞 非阻塞 同步 异步
2. 进程模型：单进程 多进程 多线程

**PPC**

Process Per Connection - 每次有新的连接就新建一个进程

实现模式简单 比较适合服务器的连接数没那么多的情况 例如数据库服务器

不足在于：

1. fork子进程代价高
2. 父子进程通信复杂
3. 支持的并发连接数量有限

prefork：提前创建进程



**TPC**

Thread Per Connection

优势：

1. 线程更轻量级 

2. 创建消耗更少 

3. 多线程是共享进程内存空间 

4. 线程通信相比进程通信更简单

不足在于：

1. 创建线程依旧是有代价的 高并发时还是有性能问题
2. 线程间的互斥和共享又引入了复杂度 可能导致死锁
3. 多线程会出现互相影响的情况，某个线程出现异常时 可能导致整个进程退出
4. 依然存在CPU线程调度和切换代价

prethread：提前创建线程

高并发需要根据两个条件划分：连接数量，请求数量。
1. 海量连接（成千上万）海量请求：例如抢购，双十一等
2. 常量连接（几十上百）海量请求：例如中间件
3. 海量连接常量请求：例如门户网站
4. 常量连接常量请求：例如内部运营系统，管理系统

> BIO(Block-IO)：一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下，在Linux系统中默认使用这种方式。可以适用于小到中规模的客户端并发数场景，无法胜任大规模并发业务。如果编程控制不善，可能造成系统资源耗尽

> NIO(NonBlock-IO)：利用多路复用IO技术，可以通过少量的线程处理大量的请求。Tomcat8在Linux系统中默认使用这种方式。Tomcat7必须修改Connector配置来启动。
> NIO最适用于“高并发”的业务场景，所谓高并发一般是指1ms内至少同时有成百上千个连接请求准备就绪，其他情况下NIO技术发挥不出它明显的优势

### 19.单服务器高性能模式 - Reactor与Proactor

> IO操作分两个阶段 
>
> 1、等待数据准备好(读到内核缓存) 
>
> 2、将数据从内核读到用户空间(进程空间)
>
>  一般来说1花费的时间远远大于2
>
> 1上不做等待就是非阻塞 NIO
>
> 2上不做等待就是异步 AIO
>
>  1上阻塞2上也阻塞的是同步阻塞IO 
>
> 1上非阻塞2阻塞的是同步非阻塞IO，这讲说的Reactor就是这种模型 
>
> 1上非阻塞2上非阻塞是异步非阻塞IO，这讲说的Proactor模型就是这种模型



Reactor：来了事件 我通知你 你来处理

Proactor：来了事件我处理 处理完了我通知你

我 - 操作系统内核

事件 - 新连接、有数据可读可写等I/O事件

你 - 我们的程序代码



**Reactor** - I/P多路复用结合线程池

核心组成：Reactor 资源池

Reactor：负责监听和分配事件

资源池：负责处理事件

常见Reactor模式的分类：

1. 单Reactor 单进程 / 线程
2. 单Reactor多线程
3. 多Reactor多进程 / 线程



**Proactor**

非阻塞异步网络模型

### 20 - 高性能负载均衡 分类和架构

> 单服务器无论如何优化，无论采用多好的硬件，总会有一个性能天花板，当单服务器的性能无法满足业务需求时，就需要设计高性能集群来提升系统整体的处理性能
>
> 高性能集群的本质很简单，通过增加更多的服务器来提升系统整体的计算能力。由于计算本身存在一个特点：**同样的输入数据和逻辑，无论在哪台服务器上执行，都应该得到相同的输出**。因此高性能集群设计的复杂度主要体现在任务分配这部分，需要设计合理的任务分配策略，将计算任务分配到多台服务器上执行

高性能集群的复杂性在于：增加了一个任务分配器，以及如何为任务分配器选择一个合适的任务分配算法

任务分配器(负载均衡器)，不只是为了计算单元的负载达到均衡状态 

有可能有的基于负载考虑 有的基于性能，有的基于业务

####负载均衡分类

**1.DNS负载均衡**

一般用于地理级别的均衡 例如 北方的用户访问北京的机房 南方的用户访问深圳的机房

本质是DNS解析同一个域名可以返回不同的IP地址

优点：

1. 简单 成本低 - 由DNS服务器处理 无须自己开发或者维护负载均衡设备
2. 就近访问 提升访问速度 

缺点：

1. 更新不及时 - DNS缓存时间比较长，修改配置后 会访问到修改前IP 可能会失败
2. 扩展性差 - 无法根据业务特点做定制化功能和扩展特性
3. 分配策略比较简单 - 支持的算法少 不能区分服务器的差异 无法感知后端服务器的状态



**2.硬件负载均衡**

常见硬件负载均衡设备：F5和A10

优点：

<ul>
<li><p>功能强大：全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡。</p>
</li>
<li><p>性能强大：对比一下，软件负载均衡支持到 10 万级并发已经很厉害了，硬件负载均衡可以支持 100 万以上的并发。</p>
</li>
<li><p>稳定性高：商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高。</p>
</li>
<li><p>支持安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙、防 DDoS 攻击等安全功能。</p>
</li>
</ul>

缺点：

<ul>
<li><p>价格昂贵：最普通的一台 F5 就是一台“马 6”，好一点的就是“Q7”了。</p>
</li>
<li><p>扩展能力差：硬件设备，可以根据业务进行配置，但无法进行扩展和定制。</p>
</li>
</ul>



**3.软件负载均衡**

通过负载均衡软件来实现负载均衡功能 常见的油Nginx和LVS

Nginx -软件的 7层负载均衡

LVS - Linux内核的 4层负载均衡

> 4层和7层的区别在于协议和灵活性
>
> Nginx 支持 HTTP、E-mail 协议；而 LVS 是 4 层负载均衡，和协议无关，几乎所有应用都可以做，例如，聊天、数据库等

软件和硬件的最主要区别在于性能 硬件的性能远远高于软件的

优点：

<ul>
<li><p>简单：无论是部署还是维护都比较简单。</p>
</li>
<li><p>便宜：只要买个 Linux 服务器，装上软件即可。</p>
</li>
<li><p>灵活：4 层和 7 层负载均衡可以根据业务进行选择；也可以根据业务进行比较方便的扩展，例如，可以通过 Nginx 的插件来实现业务的定制化功能。</p>
</li>
</ul>

缺点 - 和硬件负载均衡相比：

<ul>
<li><p>性能一般：一个 Nginx 大约能支撑 5 万并发。</p>
</li>
<li><p>功能没有硬件负载均衡那么强大。</p>
</li>
<li><p>一般不具备防火墙和防 DDoS 攻击等安全功能。</p>
</li>
</ul>



**负载均衡的典型架构**

经典架构是组合使用：DNS负载均衡用于实现地理级别的负载均衡 硬件负载均衡用于实现集群级别的负载均衡 软件负载均衡用于实现机器级别的负载均衡

即是：

![img](/Users/qspeng/Documents/learningNotes/Architecture101/pics/fzjh.png)



DNS - F5 - Nginx - Specific Instance

TPS - T: transaction 表示写请求

QPS - Q: query 表示读请求

DAU - day active user

实例：设计一个日活跃1000W的论坛

>日活千万的论坛，这个流量不低了。 
>1、首先，流量评估
>1000万DAU，换算成秒级，平均约等于116 - 1000w / (24 * 60 * 60) - 可能考虑到活跃时间，比如十小时活跃 而不是取24小时全
>
>
>考虑每个用户操作次数，假定10，换算成平均QPS=1160
>
>考虑峰值是均值倍数，假定10，换算成峰值QPS=11600
>
>考虑静态资源、图片资源、服务拆分等，流量放大效应，假定10，QPS*10=116000
>
>2、其次，容量规划
>考虑高可用、异地多活，QPS * 2=232000
>考虑未来半年增长，QPS * 1.5=348000 
>
>3、最后，方案设计
>三级导流
>第一级，DNS，确定机房，以目前量级，可以不考虑
>第二级，确定集群，扩展优先，则选Haproxy/LVS，稳定优先则选F5
>第三级，Nginx+KeepAlived，确定实例

### 21.高性能负载均衡 - 算法

算法分类：

<ul>
<li><p>任务平分类：负载均衡系统将收到的任务平均分配给服务器进行处理，这里的“平均”可以是绝对数量的平均，也可以是比例或者权重上的平均。</p>
</li>
<li><p>负载均衡类：负载均衡系统根据服务器的负载来进行分配，这里的负载并不一定是通常意义上我们说的“CPU 负载”，而是系统当前的压力，可以用 CPU 负载来衡量，也可以用连接数、I/O 使用率、网卡吞吐量等来衡量系统的压力。</p>
</li>
<li><p>性能最优类：负载均衡系统根据服务器的响应时间来进行任务分配，优先将新任务分配给响应最快的服务器。</p>
</li>
<li><p>Hash 类：负载均衡系统根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上。常见的有源地址 Hash、目标地址 Hash、session id hash、用户 ID Hash 等。</p>
</li>
</ul>

具体的算法及其优缺点：

**轮询** 

最简单 无需关注服务器本身状态

所以 不关心服务器的负载 是否异常 配置



**加权轮询**

权重一般根据硬件配置进行静态配置  动态计算会更加契合业务，但复杂度也会更高

解决不同服务器处理能力的差异

同样无法根据服务器的配置差异来进行任务分配



**负载最低优先**

分配给当前负载最低的服务器 负载根据不同的任何类型和业务场景，可以用不同的指标来衡量

例如：

1. LVS - 连接数
2. Nginx - HTTP请求数
3. 自定义 - CPU密集型：CPU负载，I/O密集型: I/O负载

缺点：

1. 如果负载均衡系统和服务器间是固定的连接池方式 无法统计建立的连接
2. CPU负载均衡需要收集每个服务器的CPU负载 而且不容易确立时间间隔
3. 算法本身可能成为性能的瓶颈



**性能最优类**

站在客户端考虑 进行分配 优先将任务分配给处理速度最快的服务器

需要感知服务器状态

复杂度：

1. 需要收集和分析每个服务器每个任务的响应时间 大量任务时，收集和统计本身也会消耗较多的性能
2. 为减少性能损耗 可采取采样的方式统计 但如何确定合适的采样率一样不容易
3. 如何选择合适的统计周期



**Hash类**

为了满足特定的业务需求 将同hash值得请求分配到同一台服务器上 - 比如抢红包

1. 原地址Hash
2. ID Hash



### 22. CAP

对于一个分布式计算系统(互相连接且共享数据的节点集合) 不可能同时满足一致性(Consistence)、可用性(Availability)、分区容错性(Partition Tolerance) 三个设计约束

只能满足三者中的两者



因为网络本身无法做到100%可靠，有可能故障。所以P是必然选择的，

可得出的选项是AP或者CP



1. CAP关注的粒度是数据 而不是整个系统
   * 实际设计过程中，每个系统不可能只处理一种数据，而是包含多种类型的数据，有的数据必须选择 CP，有的数据必须选择 AP 有的必须要CA
2. CAP是忽略网络延迟的
   * 所以C在实践中是不可能完美实现的 有些就需要选择CA - 单点写入
3. 正常运行情况下 不存在CP和AP的选择 可以同时满足CA
   * 架构设计的时候既要考虑分区发生时 选择CP 还是 AP，也要考虑没有发生时如何保证CA
4. 放弃并不等于什么都不做 需要为分区恢复后做准备



**ACID**

是数据库管理系统为了保证事务的正确性提出来的一个理论 

A - Atomicity 原子性  

C - Consistency 一致性(数据库的完整性)

I - Isolation 隔离性

D - Durabilitu 持久性

ACID是数据库事务完整性的理论



**BASE**

Basically Available - 基本可用

Soft State - 软状态

Eventual Consistency - 最终一致性

BASE 是CAP理论中AP的延伸

CAP是分布式系统设计理论









