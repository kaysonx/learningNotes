## K8S<KUBERNETES>
Service - 逻辑层 涉及网络层较多

与service密不可分的就是kube-proxy  - 现在默认只维护iptables，不做真正的流量的转发

Lsof -i:port

kube-proxy 在iptables里面<所有节点>加入路由 从service ip + port -> 转发到真正的node ip<iptables只支持随机负载均衡>

IPVS - 新的kube-proxy使用的  支持更多负载均衡方法

service数量过大时，匹配iptables条目消耗的时间就很多 -- 性能开销很大

而且当kube-proxy更新iptables时<取出全量 内存更新>，性能问题 -- 网络性能



Namespace - 隔离作用，集群中互相隔离  比如环境的区分 namespace dev sys prod， 但一般都建立不同的集群

ResourceQuota - 计算资源配额 CPU Memory 以及 K8S元素数量配额<pod svc rs..>

kubectl get ns

meta里面定义ns 默认是default

删除ns 里面所有资源都会被删除

> ClusterIP: 为Service分配虚拟地址，但是该地址只能在Kubernetes集群内部中访问,这也是Services的默认类型    
>
> NodePort: 在ClusterIP基础上将服务绑定到集群节点的指定端口上，可以通过<NodeIP>:NodePort访问
>
> Service LoadBalancer:在NodePort的基础上使用外部的负载均衡服务，通过该负载均衡服务来访问集群内部的Services,实际上经过了两层转发
>
> Node port 和 loadbalancer支持将service暴露到公网IP

Ingress -- 解决service的短板：只能面向L4 基于ingress 可以方便的创建 LV7的网络服务

K8S Ingress & Nginx ingress controller

原生Service:

clusterIp： 可以通过kubectl proxy命令 --代理svc出去 临时debug使用

nodeport：所有需要对外暴露的服务都要在**所有node**上映射一个端口

loadblancer：所有需要对外暴露的服务都需要一个loadblancer类型的svc -- 在公有云上通过loadblancer<eg ELB>打通

只需要一个service能代理后端所有的service就可以了 类比于Nginx做入口 反向代理

集群边界入口Service  --> 代理组件<比如Nginx HAproxy> --> 映射到后端的pod/service上<先到cluster ip --> pod ip>

期望的是省略service层的一次ip转发 那这样就需要proxy组件实时更新配置各个pod信息<服务发现>

Ingress 也是一个组件/资源 - 将外部的流量导入到内部 支持HTTP 7层 以及 TCP 4层

它的工作其实是一个mapping作用 比如 /api --> serviceA

基于ns使用 因为svc是分ns的  可以基于path/host来mapping

K8S中所有的resource 都是有一个controller来对应的(来落地 、操作)， yml定义的只是存在etcd的逻辑信息

所以有对应的ingress的controller -- 将 mapping关系渲染到proxy组件的配置文件中去

Ingress controller: 常用的Ingress Nginx - 不断的监听pod/service的信息 然后渲染到nginx 配置文件中去

还有比如 HAProxy  Istio<service mesh> Ingress GCE<Google Cloud>

Ingress Nginx 直接通过nginx的负载均衡算法 流量到pod级别

Service只是给controller使用来区分



Kubectl explain <resource> -可以查看这个资源的写法 在线文档

eg: kubectl explain ingress

kubectl explain ingress.spec.host - 可以渗透底层property

kubectl apply 

ll 命令

history 命令 然后 grep 或者 cmd+F



HTTPS:

 创建TLS SECRET类型的资源 然后放到pod<ingress>中 会自动调用



网络 calico

Network policy 也是存在etcd中 -可以共用k8s的etcd 或者 单独使用自己的

通信:

同一子网 bdp协议 - calico-x

跨子网 ipip - tun0

同样有集群过大 网络性能(iptables) 有可供使用的中间件 calico-typhas



Node: 

网卡 calico-x tun0 — node上

calico容器: BIRD FLEX CONFD -- 容器里面包含的组件



route -n 



使用ns进行网络隔离



K8S - NetworkPolicy组件





















 


